{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBNx_B8hz-_K"
      },
      "source": [
        "# Establishing Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvcZ5MIm1CL8"
      },
      "source": [
        "Now that I have the structure of the transformations that I am going to apply to the data, I will create pipelines to facilitate the processes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pshthwdj0sT_"
      },
      "source": [
        "## Data separation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This flag indicates whether accuracy tests will be performed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Waz2BVeVlOOK"
      },
      "outputs": [],
      "source": [
        "test = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNv9JFiQ0KlO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Read the data\n",
        "route_employee_survey = './datasets/employee_survey_data.csv'\n",
        "route_general_data = './datasets/general_data.csv'\n",
        "route_manager_survey = './datasets/manager_survey_data.csv'\n",
        "route_in_time = './datasets/in_time.csv'\n",
        "route_out_time = './datasets/out_time.csv'\n",
        "\n",
        "# Transform data into DataFrame\n",
        "employee_survey = pd.read_csv(route_employee_survey, index_col=\"EmployeeID\")\n",
        "general_data = pd.read_csv(route_general_data, index_col=\"EmployeeID\")\n",
        "manager_survey = pd.read_csv(route_manager_survey, index_col=\"EmployeeID\")\n",
        "\n",
        "in_time = pd.read_csv(route_in_time, index_col=0)\n",
        "out_time = pd.read_csv(route_out_time, index_col=0)\n",
        "\n",
        "# Set the index name to EmployeeID\n",
        "in_time.index.name = 'EmployeeID'\n",
        "out_time.index.name = 'EmployeeID'\n",
        "\n",
        "# Merge regular data sources into one dataframe\n",
        "data = employee_survey.merge(general_data, on='EmployeeID')\n",
        "data = data.merge(manager_survey, on='EmployeeID')\n",
        "\n",
        "# Add a higer index name for regular columns\n",
        "regular_cols = data.columns\n",
        "new_cols = []\n",
        "\n",
        "for col in regular_cols:\n",
        "  new_cols.append((\"Regular\", col))\n",
        "\n",
        "data.columns = pd.MultiIndex.from_tuples(new_cols)\n",
        "\n",
        "# Add a higer index name for time logs columns\n",
        "time_cols = in_time.columns\n",
        "new_in_cols = []\n",
        "new_out_cols = []\n",
        "\n",
        "for col in time_cols:\n",
        "  new_in_cols.append((\"InTime\", col))\n",
        "  new_out_cols.append((\"OutTime\", col))\n",
        "\n",
        "in_time.columns = pd.MultiIndex.from_tuples(new_in_cols)\n",
        "out_time.columns = pd.MultiIndex.from_tuples(new_out_cols)\n",
        "\n",
        "# Merge time logs data sources into dataframe\n",
        "data = data.merge(in_time, on='EmployeeID')\n",
        "data = data.merge(out_time, on='EmployeeID')\n",
        "\n",
        "# Drop rows with null values of attrition\n",
        "data.Regular = data.Regular.dropna(subset=['Attrition'])\n",
        "\n",
        "# Separate target from predictors\n",
        "y = data.Regular.Attrition\n",
        "X = data.drop([('Regular', 'Attrition')], axis=1)\n",
        "\n",
        "# Select numerical columns\n",
        "columns_numerical = X.Regular.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Select categorical columns\n",
        "columns_categorical = X.Regular.select_dtypes(include=[\"object\"]).columns\n",
        "\n",
        "# For Ordinal Encoding\n",
        "columns_ordinal = [\"BusinessTravel\", \"MaritalStatus\", \"Gender\", \"Over18\"]\n",
        "\n",
        "#For OneHot Encoding\n",
        "columns_one_hot = columns_categorical.drop(columns_ordinal)\n",
        "\n",
        "# Select numerical columns except TotalWorkingYears\n",
        "data_int_cols = columns_numerical.drop(\"TotalWorkingYears\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cruujxpn36TW"
      },
      "source": [
        "## Defining Preprocesing steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1siPUQk54I7L"
      },
      "source": [
        "### Numerical imputation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6XFCJYRgQir"
      },
      "source": [
        "Creating a custom transformer for using with TotalWorkingYears column. In rows with a null value of TotalWorkingYears, replace this value with YearsAtCompany."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_F3filZDgNQL"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "class WorkingYearsTransformer:\n",
        "    def __init__(self):\n",
        "        self.gen_imputer = SimpleImputer(strategy='median')\n",
        "\n",
        "    def fit(self, X, Y=None):\n",
        "        self.gen_imputer.fit(X[data_int_cols])\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, df):\n",
        "        transformed_df = df.copy()\n",
        "\n",
        "        transformed_df[data_int_cols] = self.gen_imputer.transform(df[data_int_cols])\n",
        "\n",
        "        # Substitution of nulls\n",
        "        transformed_df.loc[transformed_df.TotalWorkingYears.isna(),\n",
        "                           \"TotalWorkingYears\"] = transformed_df.loc[transformed_df.TotalWorkingYears.isna(),\n",
        "                                                                     \"YearsAtCompany\"]\n",
        "\n",
        "        return transformed_df\n",
        "\n",
        "\n",
        "numerical_transformer = WorkingYearsTransformer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-g-d1pB4M8v"
      },
      "source": [
        "### Categorical imputation and encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_HHniO550B9"
      },
      "outputs": [],
      "source": [
        "# Imputer for categorical columns\n",
        "\n",
        "cat_imputer = SimpleImputer(strategy='most_frequent')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LeGl_Bi6RyG"
      },
      "source": [
        "Preprocessing for categorical Ordinal data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adpH2T350KqH"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# Encoder for marital status, travel frequency and over 18.\n",
        "ordinal_encoder_travel = OrdinalEncoder(\n",
        "    categories=[np.array(['Non-Travel', 'Travel_Rarely', 'Travel_Frequently'])],\n",
        "    handle_unknown='use_encoded_value',\n",
        "    unknown_value=-1\n",
        "    )\n",
        "ordinal_encoder_marital = OrdinalEncoder(\n",
        "    categories=[np.array(['Single', 'Married', 'Divorced'])],\n",
        "    handle_unknown='use_encoded_value',\n",
        "    unknown_value=-1\n",
        "    )\n",
        "ordinal_encoder_18 = OrdinalEncoder(\n",
        "    categories=[np.array(['N', 'Y'])],\n",
        "    handle_unknown='use_encoded_value',\n",
        "    unknown_value=-1\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMCyVIZeEfz6"
      },
      "source": [
        "A custom transformer for ordinal categorical data to distinguish between the columns BusinessTravel, MaritalStatus and Over18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KGs1t447P1r"
      },
      "outputs": [],
      "source": [
        "class Categorical_Ordinal_Transformer:\n",
        "    def __init__(self):\n",
        "        self.temp_imputer = SimpleImputer(strategy='most_frequent')\n",
        "        self.ordinal_encoder_travel = ordinal_encoder_travel\n",
        "        self.ordinal_encoder_marital = ordinal_encoder_marital\n",
        "        self.ordinal_encoder_gender = OrdinalEncoder()\n",
        "        self.ordinal_encoder_18 = ordinal_encoder_18\n",
        "\n",
        "    def fit(self, X, Y=None):\n",
        "        imputed_X = X[columns_ordinal].copy()\n",
        "\n",
        "        self.temp_imputer.fit(imputed_X)\n",
        "\n",
        "        imputed_X.iloc[:, :] = self.temp_imputer.transform(imputed_X)\n",
        "\n",
        "        self.ordinal_encoder_travel.fit(imputed_X[[\"BusinessTravel\"]])\n",
        "        self.ordinal_encoder_marital.fit(imputed_X[[\"MaritalStatus\"]])\n",
        "        self.ordinal_encoder_gender.fit(imputed_X[[\"Gender\"]])\n",
        "        self.ordinal_encoder_18.fit(imputed_X[[\"Over18\"]])\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, df):\n",
        "        transformed_df = df[columns_ordinal].copy()\n",
        "\n",
        "        transformed_df.iloc[:, :] = self.temp_imputer.transform(transformed_df)\n",
        "\n",
        "        transformed_df.loc[:, \"BusinessTravel\"] = self.ordinal_encoder_travel.transform(transformed_df[[\"BusinessTravel\"]])\n",
        "        transformed_df.loc[:, \"MaritalStatus\"] = self.ordinal_encoder_marital.transform(transformed_df[[\"MaritalStatus\"]])\n",
        "        transformed_df.loc[:, \"Gender\"] = self.ordinal_encoder_gender.transform(transformed_df[[\"Gender\"]])\n",
        "        transformed_df.loc[:, \"Over18\"] = self.ordinal_encoder_18.transform(transformed_df[[\"Over18\"]])\n",
        "\n",
        "        return transformed_df\n",
        "\n",
        "\n",
        "categorical_ordinal_transformer = Categorical_Ordinal_Transformer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAkWUrS-6f2a"
      },
      "source": [
        "Preprocessing for categorical not Ordinal data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9AKszpq6d51"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "onehot_encoder = OneHotEncoder(\n",
        "    handle_unknown='infrequent_if_exist',\n",
        "    sparse_output=False,\n",
        "    )\n",
        "\n",
        "categorical_oneHot_transformer = Pipeline(steps=[\n",
        "    ('imputer', cat_imputer),\n",
        "    ('oneHot', onehot_encoder)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqCJzjL46-QS"
      },
      "source": [
        "Ordinal and OneHot separation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEMZqDlPy037"
      },
      "source": [
        "Needed a custom transformer here to manage the loss of the columns for the inner transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sd3lAQFAFdND"
      },
      "outputs": [],
      "source": [
        "class Preprocesor_Transformer:\n",
        "    def __init__(self):\n",
        "        self.categorical_ordinal_transformer = categorical_ordinal_transformer\n",
        "        self.categorical_oneHot_transformer = categorical_oneHot_transformer\n",
        "\n",
        "    def fit(self, X, Y=None):\n",
        "        ordinal_df = X[columns_ordinal]\n",
        "        oneHot_df = X[columns_one_hot]\n",
        "\n",
        "        self.categorical_ordinal_transformer = self.categorical_ordinal_transformer.fit(ordinal_df)\n",
        "        self.categorical_oneHot_transformer = self.categorical_oneHot_transformer.fit(oneHot_df)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, df):\n",
        "        transformed_df = df.copy()\n",
        "\n",
        "        transformed_df.loc[:, columns_ordinal] = self.categorical_ordinal_transformer.transform(transformed_df[columns_ordinal])\n",
        "\n",
        "        hot_encoded_data = self.categorical_oneHot_transformer.transform(transformed_df[columns_one_hot])\n",
        "        hot_encoded_columns = categorical_oneHot_transformer.get_feature_names_out()\n",
        "\n",
        "        hot_encoded_df = pd.DataFrame(hot_encoded_data, columns=hot_encoded_columns, index=transformed_df.index)\n",
        "        transformed_df = pd.concat([transformed_df[columns_ordinal], hot_encoded_df], axis=1)\n",
        "\n",
        "        return transformed_df\n",
        "\n",
        "\n",
        "categorical_transformer = Preprocesor_Transformer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OexHPRTs4f8b"
      },
      "source": [
        "### Categorical and Numerical separation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHg5In8KzTwb"
      },
      "source": [
        "Same as before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_r6nqyGy2fVX"
      },
      "outputs": [],
      "source": [
        "class Cat_Num_Separator:\n",
        "    def __init__(self):\n",
        "        self.numerical_transformer = numerical_transformer\n",
        "        self.categorical_transformer = categorical_transformer\n",
        "\n",
        "    def fit(self, X, Y=None):\n",
        "        num_df = X[columns_numerical]\n",
        "        cat_df = X[columns_categorical]\n",
        "\n",
        "        self.numerical_transformer = self.numerical_transformer.fit(num_df)\n",
        "        self.categorical_transformer = self.categorical_transformer.fit(cat_df)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, df):\n",
        "        num_vals = self.numerical_transformer.transform(df[columns_numerical])\n",
        "        cat_vals = self.categorical_transformer.transform(df[columns_categorical])\n",
        "\n",
        "        return num_vals.merge(cat_vals, on=\"EmployeeID\")\n",
        "\n",
        "\n",
        "catNumSeparator = Cat_Num_Separator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "num7UeVtZzVB"
      },
      "source": [
        "### Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSoCr2VHbfje"
      },
      "outputs": [],
      "source": [
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDeU3B6kyagJ"
      },
      "source": [
        "A customized transformer to apply dimensionality reduction with PCA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zvkBFUAb4jb"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "class Scale_Reduce:\n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        self.pca = None\n",
        "\n",
        "    def fit(self, df, Y=None):\n",
        "        self.pca = PCA(n_components=(df.shape[1]-1))\n",
        "\n",
        "        self.scaler.fit(df)\n",
        "        scaled_df = pd.DataFrame(self.scaler.transform(df), columns=df.columns)\n",
        "\n",
        "        self.pca.fit(scaled_df)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, df):\n",
        "\n",
        "        return self.pca.transform(df)\n",
        "\n",
        "\n",
        "scaleReduce = Scale_Reduce()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zySx8yHMy-gr"
      },
      "source": [
        "This function transforms the information from the in_time and out_time datasets into columns more relevant to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcNjWC02t8Ef"
      },
      "outputs": [],
      "source": [
        "def workLogsData(df_time_logs):\n",
        "  # Determines the number of days the worker was absent.\n",
        "  working_days_out_df = pd.DataFrame(df_time_logs.InTime.isna().sum(axis=1), columns=['WorkingDaysOut'])\n",
        "\n",
        "  # Transforms data to datetime type.\n",
        "  in_time_datetimes = df_time_logs.InTime.apply(pd.to_datetime)\n",
        "  out_time_datetimes = df_time_logs.OutTime.apply(pd.to_datetime)\n",
        "\n",
        "  # Calculates the time between entry and exit.\n",
        "  time_worked_datedif = out_time_datetimes - in_time_datetimes\n",
        "\n",
        "  # Determine the hours worked each day.\n",
        "  time_worked_hours_df = time_worked_datedif.applymap(lambda x: x.seconds / 3600)\n",
        "\n",
        "  # Calculates the average hours worked.\n",
        "  prom_day_hours_df = pd.DataFrame(time_worked_hours_df.apply(np.mean, axis=1), columns=['PromDayHours'])\n",
        "\n",
        "  return prom_day_hours_df.merge(working_days_out_df, on='EmployeeID')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL1ba6gz0zO3"
      },
      "source": [
        "### Final Preprocessing Step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LdBUUQR1COw"
      },
      "source": [
        "Columns that will be ignored by the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lk9Nf53gfaG5"
      },
      "outputs": [],
      "source": [
        "columns_to_drop = ['EmployeeCount', 'StandardHours', 'Over18', 'DistanceFromHome',\n",
        "                   'Education', 'JobLevel', 'StockOptionLevel', 'JobInvolvement',\n",
        "                   'JobRole_Laboratory Technician', 'EducationField_Marketing',\n",
        "                   'EducationField_Medical', 'JobRole_Healthcare Representative',\n",
        "                   'JobRole_Human Resources', 'JobRole_Manager', 'JobRole_Sales Executive',\n",
        "                   'JobRole_Sales Representative', 'Gender', 'Department_Research & Development',\n",
        "                   'EducationField_Life Sciences']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbim_ibK1Js8"
      },
      "source": [
        "Preprocessor that applies all of the above steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2p6xYUhIfKSv"
      },
      "outputs": [],
      "source": [
        "class Preprocesor_Transformer:\n",
        "    def __init__(self):\n",
        "        self.separator = Cat_Num_Separator()\n",
        "        self.pca_1 = Scale_Reduce()\n",
        "        self.pca_2 = Scale_Reduce()\n",
        "        self.pca_3 = Scale_Reduce()\n",
        "\n",
        "    def fit(self, X, Y=None):\n",
        "        self.separator.fit(X.Regular)\n",
        "        procesed_df = self.separator.transform(X.Regular)\n",
        "\n",
        "        # Columns that will reduce their dimensionality using PCA\n",
        "        self.pca_1.fit(procesed_df[[\"PerformanceRating\", \"PercentSalaryHike\"]])\n",
        "        self.pca_2.fit(procesed_df[[\"TotalWorkingYears\", \"YearsAtCompany\", \"YearsSinceLastPromotion\",\n",
        "                                    \"YearsWithCurrManager\", \"Age\"]])\n",
        "        self.pca_3.fit(procesed_df[[\"Department_Human Resources\", \"EducationField_Human Resources\"]])\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, df):\n",
        "        procesed_df = self.separator.transform(df.Regular)\n",
        "\n",
        "        pca_1 = self.pca_1.transform(procesed_df[[\"PerformanceRating\", \"PercentSalaryHike\"]])\n",
        "        pca_2 = self.pca_2.transform(procesed_df[[\"TotalWorkingYears\", \"YearsAtCompany\",\n",
        "                                                  \"YearsSinceLastPromotion\",  \"YearsWithCurrManager\",\n",
        "                                                  \"Age\"]])\n",
        "        pca_3 = self.pca_3.transform(procesed_df[[\"Department_Human Resources\", \"EducationField_Human Resources\"]])\n",
        "\n",
        "        # Removes unnecessary columns from the DataFrame.\n",
        "        processed_df = procesed_df.drop(columns_to_drop, axis=1)\n",
        "\n",
        "        # Removes reduced columns from the DataFrame.\n",
        "        processed_df = processed_df.drop([\"PerformanceRating\", \"PercentSalaryHike\", \"TotalWorkingYears\",\n",
        "                                          \"YearsAtCompany\", \"YearsSinceLastPromotion\", \"YearsWithCurrManager\",\n",
        "                                          \"Age\", \"Department_Human Resources\", \"EducationField_Human Resources\"],\n",
        "                                         axis=1)\n",
        "\n",
        "        # Transform the in_time and out_time data\n",
        "        work_logs = workLogsData(df)\n",
        "\n",
        "        return np.concatenate((processed_df, work_logs, pca_1, pca_2, pca_3), axis=1)\n",
        "\n",
        "\n",
        "preprocessor = Preprocesor_Transformer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_QExNucUrLN"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0-TORObmHYj"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier(n_estimators = 300, max_features=0.4, max_depth=15,\n",
        "                                 min_samples_leaf=2, random_state=0)\n",
        "\n",
        "aplication = Pipeline(steps=[\n",
        "    (\"preprocesor\", preprocessor),\n",
        "    (\"model\", model, )\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sbR4QBimTDD"
      },
      "source": [
        "## Model for tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kslKSqgSmz2k"
      },
      "source": [
        "### Model Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06JKGnCkm9_J",
        "outputId": "9b0bbd3f-4515-4f59-91b1-540e7156c9f2"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "if test:\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(X, y, train_size= 0.80, random_state=0);\n",
        "  aplication.fit(X_train, Y_train)\n",
        "  predicts = aplication.predict(X_test)\n",
        "\n",
        "  print(classification_report(Y_test, predicts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "CrDM6c50nYSS",
        "outputId": "a3618197-365f-4ca9-85ec-a20679ab9760"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "if test:\n",
        "  conf_mat = confusion_matrix(Y_test, predicts)\n",
        "\n",
        "  plt.figure(figsize=(6,4))\n",
        "  sns.heatmap(conf_mat, annot=True, cmap=plt.cm.Greens, linewidths=0.2, fmt='g')\n",
        "\n",
        "  # Add labels to the plot\n",
        "  class_names = [\"0\", \"1\"]\n",
        "  tick_marks = np.arange(len(class_names))\n",
        "  tick_marks2 = tick_marks + 0.5\n",
        "  plt.xticks(tick_marks2, class_names, rotation=0)\n",
        "  plt.yticks(tick_marks2, class_names, rotation=0)\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.ylabel('True label')\n",
        "  plt.title('Confusion Matrix for Random Forest Model')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJQjjIhLmdG5"
      },
      "source": [
        "### Model Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8DQciHbmVV5",
        "outputId": "90baf999-2124-457a-f3ca-5cbf58bc0cb9"
      },
      "outputs": [],
      "source": [
        "if test:\n",
        "\n",
        "  from sklearn.model_selection import cross_val_score\n",
        "  from sklearn.metrics import make_scorer, f1_score\n",
        "\n",
        "  accuracy_scores = cross_val_score(aplication, X, y,\n",
        "                                cv=5,\n",
        "                                scoring='accuracy'\n",
        "                          )\n",
        "  f1_scores = cross_val_score(aplication, X, y,\n",
        "                                cv=5,\n",
        "                                scoring=make_scorer(f1_score, greater_is_better=True, pos_label='Yes')\n",
        "                          )\n",
        "\n",
        "  print(\"Accuracy Score:\\n\", accuracy_scores)\n",
        "  print(\"F1 Score:\\n\", f1_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eyvGO2VL88j"
      },
      "source": [
        "## Model for Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_N_2FYA9ozDQ"
      },
      "outputs": [],
      "source": [
        "if not test:\n",
        "\n",
        "  aplication.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzk1S-nFo6zM"
      },
      "outputs": [],
      "source": [
        "if not test:\n",
        "\n",
        "  feat_importances = pd.Series(model.feature_importances_)\n",
        "  feat_importances.plot(kind='barh')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "pshthwdj0sT_",
        "cruujxpn36TW",
        "1siPUQk54I7L",
        "t-g-d1pB4M8v",
        "OexHPRTs4f8b",
        "num7UeVtZzVB",
        "XL1ba6gz0zO3"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
